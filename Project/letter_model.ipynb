{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset, Subset, SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a list of peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptide_score_file = './data/training_peptide_PSSM_scores.txt'\n",
    "\n",
    "def load_peptide_data(infile):\n",
    "\n",
    "    peptides = list()\n",
    "    PSSM_score = list()\n",
    "\n",
    "    with open(infile) as f:\n",
    "        for line in f:\n",
    "            peptide, score = line.strip().split()\n",
    "            peptides.append(list(peptide))\n",
    "            PSSM_score.append(float(score))\n",
    "\n",
    "    return peptides, PSSM_score\n",
    "\n",
    "peptides, PSSM_score = load_peptide_data(peptide_score_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding of the pepetides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_peptides(peptides):\n",
    "    flattened_peptides = np.array(peptides).flatten().reshape(-1, 1)\n",
    "\n",
    "    # Initialize the OneHotEncoder\n",
    "    encoder = OneHotEncoder(categories=[list('ACDEFGHIKLMNPQRSTVWY')], sparse_output=False)\n",
    "\n",
    "    # Transform the peptide sequences into a one-hot encoded format\n",
    "    one_hot_encoded = encoder.fit_transform(flattened_peptides)\n",
    "\n",
    "    # Reshape back into the original peptide sequence format\n",
    "    num_peptides = len(peptides)\n",
    "    peptide_length = len(peptides[0])\n",
    "    one_hot_encoded_peptides = one_hot_encoded.reshape(num_peptides, peptide_length, -1)\n",
    "\n",
    "    return one_hot_encoded_peptides\n",
    "\n",
    "one_hot_encoded_peptides = one_hot_encode_peptides(peptides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class Linear_NN(torch.nn.Module):\n",
    "\n",
    "    # 180 is given from the one-hot encoding of the 20 amino acids * 9 peptide length\n",
    "    def __init__(self):\n",
    "        super(Linear_NN, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(180, 90)\n",
    "        self.fc2 = torch.nn.Linear(90, 50)\n",
    "        self.fc3 = torch.nn.Linear(50, 1)\n",
    "        self.drop = torch.nn.Dropout(p=0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        out = self.fc3(x)\n",
    "        return out\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to reset weight\n",
    "Weight resetting aid to prevent the weight leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        print(f'reset weight of layer {m}')\n",
    "        m.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function \n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptides_tensor = torch.tensor(one_hot_encoded_peptides, dtype=torch.float32).to(device)\n",
    "PSSM_score_tensor = torch.tensor(np.asarray(PSSM_score).reshape(-1,1), dtype=torch.float32).to(device)\n",
    "\n",
    "peptides_dataset = TensorDataset(peptides_tensor, PSSM_score_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_loss, val_loss, fold):\n",
    "    plt.figure()\n",
    "    plt.plot(train_loss, label='train err')\n",
    "    plt.plot(val_loss, label='val err')\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig(f'./loss/loss_fold_{fold}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting KFold Cross Validation\n",
      "Fold 1\n",
      "reset weight of layer Linear(in_features=180, out_features=90, bias=True)\n",
      "reset weight of layer Linear(in_features=90, out_features=50, bias=True)\n",
      "reset weight of layer Linear(in_features=50, out_features=1, bias=True)\n",
      "Epoch 1\n",
      "Training loss: 19.17353608927776\n",
      "Validation Loss: 10.600921984314919\n",
      "Epoch 2\n",
      "Training loss: 2.627593762788576\n",
      "Validation Loss: 0.4102025318145752\n",
      "Epoch 3\n",
      "Training loss: 0.14514806140790282\n",
      "Validation Loss: 0.11441241659224033\n",
      "Epoch 4\n",
      "Training loss: 0.03518792810686624\n",
      "Validation Loss: 0.031000978648662567\n",
      "Epoch 5\n",
      "Training loss: 0.015912191836720276\n",
      "Validation Loss: 0.022302857376635076\n",
      "Epoch 6\n",
      "Training loss: 0.011617341844518621\n",
      "Validation Loss: 0.02071586450561881\n",
      "Epoch 7\n",
      "Training loss: 0.007633490424324787\n",
      "Validation Loss: 0.017322564404457806\n",
      "Epoch 8\n",
      "Training loss: 0.00894437523765163\n",
      "Validation Loss: 0.013936822889372707\n",
      "Epoch 9\n",
      "Training loss: 0.005398369931913528\n",
      "Validation Loss: 0.01404979133978486\n",
      "Epoch 10\n",
      "Training loss: 0.006434654359124869\n",
      "Validation Loss: 0.013627607682719827\n",
      "Epoch 11\n",
      "Training loss: 0.005500027901473811\n",
      "Validation Loss: 0.014024330463726073\n",
      "Epoch 12\n",
      "Training loss: 0.005487451877099337\n",
      "Validation Loss: 0.015126823456957936\n",
      "Epoch 13\n",
      "Training loss: 0.004493988073459759\n",
      "Validation Loss: 0.014264251962304115\n",
      "Epoch 14\n",
      "Training loss: 0.005745532720820191\n",
      "Validation Loss: 0.01771047377958894\n",
      "Epoch 15\n",
      "Training loss: 0.009852206296066647\n",
      "Validation Loss: 0.020580533967586235\n",
      "Epoch 16\n",
      "Training loss: 0.009458717664454094\n",
      "Validation Loss: 0.01579319080337882\n",
      "Epoch 17\n",
      "Training loss: 0.008696914297653381\n",
      "Validation Loss: 0.019517439790070056\n",
      "Epoch 18\n",
      "Training loss: 0.012878649654442962\n",
      "Validation Loss: 0.02033757415600121\n",
      "Epoch 19\n",
      "Training loss: 0.012448576120563696\n",
      "Validation Loss: 0.02539027627557516\n",
      "Epoch 20\n",
      "Training loss: 0.023060882718484735\n",
      "Validation Loss: 0.06070635750889778\n",
      "Epoch 21\n",
      "Training loss: 0.0668167447882522\n",
      "Validation Loss: 0.07221343178302049\n",
      "Epoch 22\n",
      "Training loss: 0.05285773178581724\n",
      "Validation Loss: 0.04071303363889456\n",
      "Epoch 23\n",
      "Training loss: 0.030411210866433752\n",
      "Validation Loss: 0.029279545098543167\n",
      "Epoch 24\n",
      "Training loss: 0.023384994808008375\n",
      "Validation Loss: 0.030504234060645105\n",
      "Epoch 25\n",
      "Training loss: 0.018481859534057146\n",
      "Validation Loss: 0.03284824840724468\n",
      "Epoch 26\n",
      "Training loss: 0.021443215059596545\n",
      "Validation Loss: 0.03713080234825611\n",
      "Epoch 27\n",
      "Training loss: 0.028209983947273996\n",
      "Validation Loss: 0.02992972407490015\n",
      "Epoch 28\n",
      "Training loss: 0.01576957735233009\n",
      "Validation Loss: 0.020511225727386773\n",
      "Epoch 29\n",
      "Training loss: 0.014858337236837167\n",
      "Validation Loss: 0.03710015743970871\n",
      "Epoch 30\n",
      "Training loss: 0.02311318211699116\n",
      "Validation Loss: 0.03686426920816302\n",
      "Epoch 31\n",
      "Training loss: 0.021993592199053346\n",
      "Validation Loss: 0.025977578219026326\n",
      "Epoch 32\n",
      "Training loss: 0.021444713898463964\n",
      "Validation Loss: 0.024518705224618315\n",
      "Epoch 33\n",
      "Training loss: 0.02122564311379317\n",
      "Validation Loss: 0.037880008071660996\n",
      "Epoch 34\n",
      "Training loss: 0.024275354873004956\n",
      "Validation Loss: 0.041697190403938295\n",
      "Epoch 35\n",
      "Training loss: 0.026881300829850213\n",
      "Validation Loss: 0.030834480933845044\n",
      "Epoch 36\n",
      "Training loss: 0.022639452052523486\n",
      "Validation Loss: 0.0347150219604373\n",
      "Epoch 37\n",
      "Training loss: 0.014913700753822923\n",
      "Validation Loss: 0.03453275240957737\n",
      "Epoch 38\n",
      "Training loss: 0.017256764277393363\n",
      "Validation Loss: 0.025539518389850854\n",
      "Epoch 39\n",
      "Training loss: 0.016079111653620127\n",
      "Validation Loss: 0.033303168686106804\n",
      "Epoch 40\n",
      "Training loss: 0.021830595693714226\n",
      "Validation Loss: 0.03688692506402731\n",
      "Epoch 41\n",
      "Training loss: 0.01852627117522829\n",
      "Validation Loss: 0.03642457671463489\n",
      "Epoch 42\n",
      "Training loss: 0.01935417514736044\n",
      "Validation Loss: 0.02968275305815041\n",
      "Epoch 43\n",
      "Training loss: 0.015521101730386006\n",
      "Validation Loss: 0.02428145844489336\n",
      "Epoch 44\n",
      "Training loss: 0.017412896080845102\n",
      "Validation Loss: 0.033598640598356724\n",
      "Epoch 45\n",
      "Training loss: 0.017374024985684564\n",
      "Validation Loss: 0.02792211502790451\n",
      "Epoch 46\n",
      "Training loss: 0.013501125575072066\n",
      "Validation Loss: 0.02786081288009882\n",
      "Epoch 47\n",
      "Training loss: 0.011596064378517037\n",
      "Validation Loss: 0.021830990659072996\n",
      "Epoch 48\n",
      "Training loss: 0.015157712093007165\n",
      "Validation Loss: 0.027609492130577565\n",
      "Epoch 49\n",
      "Training loss: 0.014730553553180443\n",
      "Validation Loss: 0.027694850303232668\n",
      "Epoch 50\n",
      "Training loss: 0.022664358959408458\n",
      "Validation Loss: 0.03404462646692991\n",
      "Epoch 51\n",
      "Training loss: 0.022327857956136624\n",
      "Validation Loss: 0.03040702996775508\n",
      "Epoch 52\n",
      "Training loss: 0.013171320665896553\n",
      "Validation Loss: 0.027186132781207562\n",
      "Epoch 53\n",
      "Training loss: 0.013239470874583445\n",
      "Validation Loss: 0.02949160736054182\n",
      "Epoch 54\n",
      "Training loss: 0.013227783401554316\n",
      "Validation Loss: 0.029726691115647554\n",
      "Epoch 55\n",
      "Training loss: 0.011637579640085549\n",
      "Validation Loss: 0.021428812865924556\n",
      "Epoch 56\n",
      "Training loss: 0.01062998951403136\n",
      "Validation Loss: 0.02968724936246872\n",
      "Epoch 57\n",
      "Training loss: 0.017305702935981073\n",
      "Validation Loss: 0.0396914067491889\n",
      "Epoch 58\n",
      "Training loss: 0.017458255349936867\n",
      "Validation Loss: 0.03148668427020311\n",
      "Epoch 59\n",
      "Training loss: 0.01718158002453136\n",
      "Validation Loss: 0.033722124453634025\n",
      "Epoch 60\n",
      "Training loss: 0.02713062594999972\n",
      "Validation Loss: 0.04176942206919193\n",
      "Epoch 61\n",
      "Training loss: 0.02862901780658315\n",
      "Validation Loss: 0.04875139944255352\n",
      "Epoch 62\n",
      "Training loss: 0.023025663852806865\n",
      "Validation Loss: 0.03493779513984919\n",
      "Epoch 63\n",
      "Training loss: 0.02578032766612842\n",
      "Validation Loss: 0.042712507992982866\n",
      "Epoch 64\n",
      "Training loss: 0.020174341204278554\n",
      "Validation Loss: 0.030800922960042953\n",
      "Epoch 65\n",
      "Training loss: 0.009858771688160822\n",
      "Validation Loss: 0.021829435080289842\n",
      "Epoch 66\n",
      "Training loss: 0.007919683846040177\n",
      "Validation Loss: 0.01915810409002006\n",
      "Epoch 67\n",
      "Training loss: 0.006366641438306914\n",
      "Validation Loss: 0.030962812565267086\n",
      "Epoch 68\n",
      "Training loss: 0.007490452950263454\n",
      "Validation Loss: 0.02166610600426793\n",
      "Epoch 69\n",
      "Training loss: 0.006140071096993291\n",
      "Validation Loss: 0.023951420318335293\n",
      "Epoch 70\n",
      "Training loss: 0.008207808527135357\n",
      "Validation Loss: 0.02201192223466933\n",
      "Epoch 71\n",
      "Training loss: 0.009168221075183799\n",
      "Validation Loss: 0.025189829412847756\n",
      "Epoch 72\n",
      "Training loss: 0.011135950512924837\n",
      "Validation Loss: 0.026748932115733624\n",
      "Epoch 73\n",
      "Training loss: 0.01542958852921411\n",
      "Validation Loss: 0.03093673374503851\n",
      "Epoch 74\n",
      "Training loss: 0.01720076833477186\n",
      "Validation Loss: 0.029250542894005777\n",
      "Epoch 75\n",
      "Training loss: 0.017575613443845325\n",
      "Validation Loss: 0.03784979827702045\n",
      "Epoch 76\n",
      "Training loss: 0.022919693443271303\n",
      "Validation Loss: 0.053556465804576875\n",
      "Epoch 77\n",
      "Training loss: 0.03804368952999717\n",
      "Validation Loss: 0.07243051014840603\n",
      "Epoch 78\n",
      "Training loss: 0.03327655406745592\n",
      "Validation Loss: 0.054677811861038206\n",
      "Epoch 79\n",
      "Training loss: 0.014866640748091272\n",
      "Validation Loss: 0.01957966818474233\n",
      "Epoch 80\n",
      "Training loss: 0.009151336512547563\n",
      "Validation Loss: 0.022488930239342154\n",
      "Epoch 81\n",
      "Training loss: 0.007345010305649226\n",
      "Validation Loss: 0.03027488436549902\n",
      "Epoch 82\n",
      "Training loss: 0.010214254257662855\n",
      "Validation Loss: 0.02278612310066819\n",
      "Epoch 83\n",
      "Training loss: 0.006567015317412689\n",
      "Validation Loss: 0.026482211090624333\n",
      "Epoch 84\n",
      "Training loss: 0.008335347517787181\n",
      "Validation Loss: 0.018780289962887765\n",
      "Epoch 85\n",
      "Training loss: 0.007924501713336512\n",
      "Validation Loss: 0.02921294007450342\n",
      "Epoch 86\n",
      "Training loss: 0.013313730415340858\n",
      "Validation Loss: 0.03407742097973823\n",
      "Epoch 87\n",
      "Training loss: 0.013647562240912896\n",
      "Validation Loss: 0.02692347472300753\n",
      "Epoch 88\n",
      "Training loss: 0.009174502872353055\n",
      "Validation Loss: 0.024998150058090685\n",
      "Epoch 89\n",
      "Training loss: 0.008822842825146527\n",
      "Validation Loss: 0.023632037627976388\n",
      "Epoch 90\n",
      "Training loss: 0.00928542011815901\n",
      "Validation Loss: 0.01926959626376629\n",
      "Epoch 91\n",
      "Training loss: 0.012807687290838546\n",
      "Validation Loss: 0.041034857900813224\n",
      "Epoch 92\n",
      "Training loss: 0.01344213075695809\n",
      "Validation Loss: 0.04801012899726629\n",
      "Epoch 93\n",
      "Training loss: 0.02252351804684425\n",
      "Validation Loss: 0.03947461923584342\n",
      "Epoch 94\n",
      "Training loss: 0.016481371334821138\n",
      "Validation Loss: 0.02124293043743819\n",
      "Epoch 95\n",
      "Training loss: 0.018325067134862094\n",
      "Validation Loss: 0.06126310735940933\n",
      "Epoch 96\n",
      "Training loss: 0.04792075666608577\n",
      "Validation Loss: 0.04979205820709467\n",
      "Epoch 97\n",
      "Training loss: 0.023087796452695254\n",
      "Validation Loss: 0.06960438657552004\n",
      "Epoch 98\n",
      "Training loss: 0.013570802367878021\n",
      "Validation Loss: 0.020398117736913263\n",
      "Epoch 99\n",
      "Training loss: 0.006232972885121966\n",
      "Validation Loss: 0.021458946112543346\n",
      "Epoch 100\n",
      "Training loss: 0.004630541037672113\n",
      "Validation Loss: 0.01948573173955083\n",
      "Finished Training, Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chuan\\AppData\\Local\\Temp\\ipykernel_11812\\1195006754.py:7: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-whitegrid')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2\n",
      "reset weight of layer Linear(in_features=180, out_features=90, bias=True)\n",
      "reset weight of layer Linear(in_features=90, out_features=50, bias=True)\n",
      "reset weight of layer Linear(in_features=50, out_features=1, bias=True)\n",
      "Epoch 1\n",
      "Training loss: 19.254307793587753\n",
      "Validation Loss: 13.906777420043944\n",
      "Epoch 2\n",
      "Training loss: 3.3371314275817774\n",
      "Validation Loss: 0.3768458467721939\n",
      "Epoch 3\n",
      "Training loss: 0.14092301583090394\n",
      "Validation Loss: 0.09023585930466652\n",
      "Epoch 4\n",
      "Training loss: 0.03655865452895613\n",
      "Validation Loss: 0.04166515866294503\n",
      "Epoch 5\n",
      "Training loss: 0.01813762302788877\n",
      "Validation Loss: 0.03032526755705476\n",
      "Epoch 6\n",
      "Training loss: 0.013927330773026134\n",
      "Validation Loss: 0.029818616807460785\n",
      "Epoch 7\n",
      "Training loss: 0.010436875757655686\n",
      "Validation Loss: 0.022294865846633913\n",
      "Epoch 8\n",
      "Training loss: 0.008419201770538138\n",
      "Validation Loss: 0.037292981017380954\n",
      "Epoch 9\n",
      "Training loss: 0.009471563838383894\n",
      "Validation Loss: 0.021565131116658448\n",
      "Epoch 10\n",
      "Training loss: 0.006805557324359021\n",
      "Validation Loss: 0.019594892682507633\n",
      "Epoch 11\n",
      "Training loss: 0.005841775464863737\n",
      "Validation Loss: 0.016849943979177624\n",
      "Epoch 12\n",
      "Training loss: 0.006618768324806672\n",
      "Validation Loss: 0.017529889419674875\n",
      "Epoch 13\n",
      "Training loss: 0.00569729878178308\n",
      "Validation Loss: 0.01770198015496135\n",
      "Epoch 14\n",
      "Training loss: 0.007046700238019766\n",
      "Validation Loss: 0.018503689458593726\n",
      "Epoch 15\n",
      "Training loss: 0.011272717174862694\n",
      "Validation Loss: 0.02241956742480397\n",
      "Epoch 16\n",
      "Training loss: 0.012977904623335939\n",
      "Validation Loss: 0.03105843987315893\n",
      "Epoch 17\n",
      "Training loss: 0.03239833478595024\n",
      "Validation Loss: 0.03922843690961599\n",
      "Epoch 18\n",
      "Training loss: 0.035905980605862496\n",
      "Validation Loss: 0.03890011528274044\n",
      "Epoch 19\n",
      "Training loss: 0.028733346376030408\n",
      "Validation Loss: 0.06474821381270886\n",
      "Epoch 20\n",
      "Training loss: 0.028676794092987002\n",
      "Validation Loss: 0.03221842115744948\n",
      "Epoch 21\n",
      "Training loss: 0.022179821630959044\n",
      "Validation Loss: 0.045662236586213115\n",
      "Epoch 22\n",
      "Training loss: 0.020041579058989116\n",
      "Validation Loss: 0.03128327626734972\n",
      "Epoch 23\n",
      "Training loss: 0.026161569473093626\n",
      "Validation Loss: 0.0290639940276742\n",
      "Epoch 24\n",
      "Training loss: 0.017227202973126906\n",
      "Validation Loss: 0.030427506119012834\n",
      "Epoch 25\n",
      "Training loss: 0.015202782863328598\n",
      "Validation Loss: 0.03976700767874718\n",
      "Epoch 26\n",
      "Training loss: 0.01864426223964421\n",
      "Validation Loss: 0.03535943642258644\n",
      "Epoch 27\n",
      "Training loss: 0.018480991731042562\n",
      "Validation Loss: 0.03476268585771322\n",
      "Epoch 28\n",
      "Training loss: 0.019900526726599206\n",
      "Validation Loss: 0.03309637946076691\n",
      "Epoch 29\n",
      "Training loss: 0.03690937018386789\n",
      "Validation Loss: 0.10299264058470727\n",
      "Epoch 30\n",
      "Training loss: 0.04259634592099903\n",
      "Validation Loss: 0.047553943991661074\n",
      "Epoch 31\n",
      "Training loss: 0.0575926938795091\n",
      "Validation Loss: 0.04997369915246964\n",
      "Epoch 32\n",
      "Training loss: 0.022862443753239727\n",
      "Validation Loss: 0.027106142453849316\n",
      "Epoch 33\n",
      "Training loss: 0.011977715619369266\n",
      "Validation Loss: 0.03146986933425069\n",
      "Epoch 34\n",
      "Training loss: 0.010030716372000956\n",
      "Validation Loss: 0.020659921616315843\n",
      "Epoch 35\n",
      "Training loss: 0.0063375428597414\n",
      "Validation Loss: 0.021112962337210776\n",
      "Epoch 36\n",
      "Training loss: 0.00989988429073398\n",
      "Validation Loss: 0.0361257603764534\n",
      "Epoch 37\n",
      "Training loss: 0.015900111041924694\n",
      "Validation Loss: 0.03093112874776125\n",
      "Epoch 38\n",
      "Training loss: 0.018238868775595093\n",
      "Validation Loss: 0.029969990060199053\n",
      "Epoch 39\n",
      "Training loss: 0.012402261247301531\n",
      "Validation Loss: 0.023121587969362735\n",
      "Epoch 40\n",
      "Training loss: 0.01298971733164772\n",
      "Validation Loss: 0.033258620612323285\n",
      "Epoch 41\n",
      "Training loss: 0.01792071423176484\n",
      "Validation Loss: 0.04772464033216238\n",
      "Epoch 42\n",
      "Training loss: 0.03176543666721927\n",
      "Validation Loss: 0.042398716527968644\n",
      "Epoch 43\n",
      "Training loss: 0.025856572210539094\n",
      "Validation Loss: 0.050866721644997594\n",
      "Epoch 44\n",
      "Training loss: 0.025488035282907412\n",
      "Validation Loss: 0.035885455869138244\n",
      "Epoch 45\n",
      "Training loss: 0.021469219738963184\n",
      "Validation Loss: 0.030902387499809267\n",
      "Epoch 46\n",
      "Training loss: 0.015935998088344165\n",
      "Validation Loss: 0.034189265593886374\n",
      "Epoch 47\n",
      "Training loss: 0.011882072811485412\n",
      "Validation Loss: 0.026620232425630094\n",
      "Epoch 48\n",
      "Training loss: 0.01348913309389975\n",
      "Validation Loss: 0.029045278802514077\n",
      "Epoch 49\n",
      "Training loss: 0.013314558029866096\n",
      "Validation Loss: 0.02453712310642004\n",
      "Epoch 50\n",
      "Training loss: 0.0173903111195595\n",
      "Validation Loss: 0.040979148745536806\n",
      "Epoch 51\n",
      "Training loss: 0.03538588996611766\n",
      "Validation Loss: 0.04605799083132297\n",
      "Epoch 52\n",
      "Training loss: 0.03909108218900168\n",
      "Validation Loss: 0.040315818823874\n",
      "Epoch 53\n",
      "Training loss: 0.017065570603326422\n",
      "Validation Loss: 0.03358644979074597\n",
      "Epoch 54\n",
      "Training loss: 0.013782818751294435\n",
      "Validation Loss: 0.025385090839117764\n",
      "Epoch 55\n",
      "Training loss: 0.01768143436170577\n",
      "Validation Loss: 0.05469879798591137\n",
      "Epoch 56\n",
      "Training loss: 0.02913048661784413\n",
      "Validation Loss: 0.03742485880851745\n",
      "Epoch 57\n",
      "Training loss: 0.013534384789233356\n",
      "Validation Loss: 0.022775827031582593\n",
      "Epoch 58\n",
      "Training loss: 0.007372332148772386\n",
      "Validation Loss: 0.022588576283305885\n",
      "Epoch 59\n",
      "Training loss: 0.008303260147456349\n",
      "Validation Loss: 0.038291162475943566\n",
      "Epoch 60\n",
      "Training loss: 0.013834637208102444\n",
      "Validation Loss: 0.025899469777941705\n",
      "Epoch 61\n",
      "Training loss: 0.016581563927118004\n",
      "Validation Loss: 0.04070907045155764\n",
      "Epoch 62\n",
      "Training loss: 0.014540673641622374\n",
      "Validation Loss: 0.028781635295599697\n",
      "Epoch 63\n",
      "Training loss: 0.009054658615231975\n",
      "Validation Loss: 0.030304459761828183\n",
      "Epoch 64\n",
      "Training loss: 0.010564211355138225\n",
      "Validation Loss: 0.03296257866546512\n",
      "Epoch 65\n",
      "Training loss: 0.028041600004875475\n",
      "Validation Loss: 0.053416656404733656\n",
      "Epoch 66\n",
      "Training loss: 0.03866944285397677\n",
      "Validation Loss: 0.058653230294585225\n",
      "Epoch 67\n",
      "Training loss: 0.023917752807748688\n",
      "Validation Loss: 0.040256495662033556\n",
      "Epoch 68\n",
      "Training loss: 0.011554025492832526\n",
      "Validation Loss: 0.020934203574433922\n",
      "Epoch 69\n",
      "Training loss: 0.007243572818307379\n",
      "Validation Loss: 0.02349137330194935\n",
      "Epoch 70\n",
      "Training loss: 0.005401923855257787\n",
      "Validation Loss: 0.0180065034609288\n",
      "Epoch 71\n",
      "Training loss: 0.006014530998222607\n",
      "Validation Loss: 0.02140658088028431\n",
      "Epoch 72\n",
      "Training loss: 0.00876860126622559\n",
      "Validation Loss: 0.051534032896161076\n",
      "Epoch 73\n",
      "Training loss: 0.018208750029161724\n",
      "Validation Loss: 0.035151570123853164\n",
      "Epoch 74\n",
      "Training loss: 0.016376162199879585\n",
      "Validation Loss: 0.029647277556359767\n",
      "Epoch 75\n",
      "Training loss: 0.013307547333563879\n",
      "Validation Loss: 0.032801748290657996\n",
      "Epoch 76\n",
      "Training loss: 0.014739331520479364\n",
      "Validation Loss: 0.0273580834409222\n",
      "Epoch 77\n",
      "Training loss: 0.02270353909524292\n",
      "Validation Loss: 0.039092916380614046\n",
      "Epoch 78\n",
      "Training loss: 0.0230595476339696\n",
      "Validation Loss: 0.025427796095609665\n",
      "Epoch 79\n",
      "Training loss: 0.01536231100098374\n",
      "Validation Loss: 0.02998243668116629\n",
      "Epoch 80\n",
      "Training loss: 0.012151748964505405\n",
      "Validation Loss: 0.026724004708230494\n",
      "Epoch 81\n",
      "Training loss: 0.011853868336195951\n",
      "Validation Loss: 0.027676668986678123\n",
      "Epoch 82\n",
      "Training loss: 0.009956366656175287\n",
      "Validation Loss: 0.02452998250722885\n",
      "Epoch 83\n",
      "Training loss: 0.015331060997661702\n",
      "Validation Loss: 0.02948244553001132\n",
      "Epoch 84\n",
      "Training loss: 0.019029640911876696\n",
      "Validation Loss: 0.030212362678721547\n",
      "Epoch 85\n",
      "Training loss: 0.013202963876970036\n",
      "Validation Loss: 0.03395024936646223\n",
      "Epoch 86\n",
      "Training loss: 0.01332679996987056\n",
      "Validation Loss: 0.030968207828700543\n",
      "Epoch 87\n",
      "Training loss: 0.011456370855364757\n",
      "Validation Loss: 0.020776995327323675\n",
      "Epoch 88\n",
      "Training loss: 0.00820686409403522\n",
      "Validation Loss: 0.019906359743326902\n",
      "Epoch 89\n",
      "Training loss: 0.009398378381390394\n",
      "Validation Loss: 0.025548545513302087\n",
      "Epoch 90\n",
      "Training loss: 0.01028494073615707\n",
      "Validation Loss: 0.031726407464593646\n",
      "Epoch 91\n",
      "Training loss: 0.011842085286187604\n",
      "Validation Loss: 0.024366781832650303\n",
      "Epoch 92\n",
      "Training loss: 0.012943741382992606\n",
      "Validation Loss: 0.02896060537546873\n",
      "Epoch 93\n",
      "Training loss: 0.023319967249504376\n",
      "Validation Loss: 0.03346668314188719\n",
      "Epoch 94\n",
      "Training loss: 0.018133503272073324\n",
      "Validation Loss: 0.027457907795906067\n",
      "Epoch 95\n",
      "Training loss: 0.017880957204003615\n",
      "Validation Loss: 0.05405606843531132\n",
      "Epoch 96\n",
      "Training loss: 0.012881976523503815\n",
      "Validation Loss: 0.026098669385537506\n",
      "Epoch 97\n",
      "Training loss: 0.00954607723535221\n",
      "Validation Loss: 0.026073974531609565\n",
      "Epoch 98\n",
      "Training loss: 0.005374466447092439\n",
      "Validation Loss: 0.022166294823400677\n",
      "Epoch 99\n",
      "Training loss: 0.01135466157545134\n",
      "Validation Loss: 0.023161102291196584\n",
      "Epoch 100\n",
      "Training loss: 0.011211047328278883\n",
      "Validation Loss: 0.022555662281811238\n",
      "Finished Training, Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chuan\\AppData\\Local\\Temp\\ipykernel_11812\\1195006754.py:7: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-whitegrid')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3\n",
      "reset weight of layer Linear(in_features=180, out_features=90, bias=True)\n",
      "reset weight of layer Linear(in_features=90, out_features=50, bias=True)\n",
      "reset weight of layer Linear(in_features=50, out_features=1, bias=True)\n",
      "Epoch 1\n",
      "Training loss: 18.85334917442086\n",
      "Validation Loss: 12.528187160491944\n",
      "Epoch 2\n",
      "Training loss: 2.713584614860028\n",
      "Validation Loss: 0.47325438261032104\n",
      "Epoch 3\n",
      "Training loss: 0.1470432456045114\n",
      "Validation Loss: 0.10383136346936225\n",
      "Epoch 4\n",
      "Training loss: 0.029699507039808427\n",
      "Validation Loss: 0.05295970883220434\n",
      "Epoch 5\n",
      "Training loss: 0.016823038643648484\n",
      "Validation Loss: 0.03517986262217164\n",
      "Epoch 6\n",
      "Training loss: 0.010889310144924932\n",
      "Validation Loss: 0.03111862888559699\n",
      "Epoch 7\n",
      "Training loss: 0.00834792059577387\n",
      "Validation Loss: 0.0318218888156116\n",
      "Epoch 8\n",
      "Training loss: 0.007173223698214081\n",
      "Validation Loss: 0.02388472360558808\n",
      "Epoch 9\n",
      "Training loss: 0.007807345183556611\n",
      "Validation Loss: 0.03730234574526548\n",
      "Epoch 10\n",
      "Training loss: 0.007270838138791397\n",
      "Validation Loss: 0.03157409846782684\n",
      "Epoch 11\n",
      "Training loss: 0.005513382497118767\n",
      "Validation Loss: 0.026698109190911055\n",
      "Epoch 12\n",
      "Training loss: 0.005573328819663562\n",
      "Validation Loss: 0.020595429446548224\n",
      "Epoch 13\n",
      "Training loss: 0.008579428497337034\n",
      "Validation Loss: 0.028924257734324783\n",
      "Epoch 14\n",
      "Training loss: 0.01336453579721454\n",
      "Validation Loss: 0.029701844193041324\n",
      "Epoch 15\n",
      "Training loss: 0.01311978200431337\n",
      "Validation Loss: 0.04125133480876684\n",
      "Epoch 16\n",
      "Training loss: 0.01701107442129365\n",
      "Validation Loss: 0.05515675336122513\n",
      "Epoch 17\n",
      "Training loss: 0.025549726930360513\n",
      "Validation Loss: 0.05265057921409607\n",
      "Epoch 18\n",
      "Training loss: 0.033174600106539186\n",
      "Validation Loss: 0.04121399287134409\n",
      "Epoch 19\n",
      "Training loss: 0.03501618105783751\n",
      "Validation Loss: 0.06255117673426866\n",
      "Epoch 20\n",
      "Training loss: 0.035072625792326076\n",
      "Validation Loss: 0.06377186328172683\n",
      "Epoch 21\n",
      "Training loss: 0.0284049034397089\n",
      "Validation Loss: 0.05043632727116346\n",
      "Epoch 22\n",
      "Training loss: 0.029954670433953556\n",
      "Validation Loss: 0.04924503661692142\n",
      "Epoch 23\n",
      "Training loss: 0.026593899191117164\n",
      "Validation Loss: 0.048241198621690275\n",
      "Epoch 24\n",
      "Training loss: 0.015560419029873056\n",
      "Validation Loss: 0.045178990662097934\n",
      "Epoch 25\n",
      "Training loss: 0.018014006290258358\n",
      "Validation Loss: 0.040220700204372406\n",
      "Epoch 26\n",
      "Training loss: 0.014656569799120278\n",
      "Validation Loss: 0.04151852589100599\n",
      "Epoch 27\n",
      "Training loss: 0.01580752684379515\n",
      "Validation Loss: 0.037958893701434133\n",
      "Epoch 28\n",
      "Training loss: 0.012346675752700527\n",
      "Validation Loss: 0.03384820271283388\n",
      "Epoch 29\n",
      "Training loss: 0.016694828119013728\n",
      "Validation Loss: 0.055391441509127615\n",
      "Epoch 30\n",
      "Training loss: 0.022238311727444844\n",
      "Validation Loss: 0.05421957328915596\n",
      "Epoch 31\n",
      "Training loss: 0.04039191038434192\n",
      "Validation Loss: 0.05388967707753181\n",
      "Epoch 32\n",
      "Training loss: 0.024057636104677756\n",
      "Validation Loss: 0.05098607128486037\n",
      "Epoch 33\n",
      "Training loss: 0.02126536680596698\n",
      "Validation Loss: 0.057605376709252594\n",
      "Epoch 34\n",
      "Training loss: 0.016458796682892386\n",
      "Validation Loss: 0.038283113408833745\n",
      "Epoch 35\n",
      "Training loss: 0.017066103804702918\n",
      "Validation Loss: 0.051562773026525975\n",
      "Epoch 36\n",
      "Training loss: 0.03994237971924169\n",
      "Validation Loss: 0.04975065803155303\n",
      "Epoch 37\n",
      "Training loss: 0.03335306462207713\n",
      "Validation Loss: 0.06086723774671555\n",
      "Epoch 38\n",
      "Training loss: 0.02354160883492723\n",
      "Validation Loss: 0.05470738722011447\n",
      "Epoch 39\n",
      "Training loss: 0.018403825119675435\n",
      "Validation Loss: 0.04061766938306391\n",
      "Epoch 40\n",
      "Training loss: 0.017802898428815696\n",
      "Validation Loss: 0.056192494872957466\n",
      "Epoch 41\n",
      "Training loss: 0.02088804908346423\n",
      "Validation Loss: 0.04740418842062354\n",
      "Epoch 42\n",
      "Training loss: 0.020410502761525593\n",
      "Validation Loss: 0.0451008128747344\n",
      "Epoch 43\n",
      "Training loss: 0.018879298598399943\n",
      "Validation Loss: 0.0491124253347516\n",
      "Epoch 44\n",
      "Training loss: 0.012760894271045847\n",
      "Validation Loss: 0.04350717280060053\n",
      "Epoch 45\n",
      "Training loss: 0.010439596083687292\n",
      "Validation Loss: 0.034710399815812705\n",
      "Epoch 46\n",
      "Training loss: 0.012095817926426217\n",
      "Validation Loss: 0.04500291120260954\n",
      "Epoch 47\n",
      "Training loss: 0.008555032857741738\n",
      "Validation Loss: 0.03841523863375187\n",
      "Epoch 48\n",
      "Training loss: 0.008414787297933986\n",
      "Validation Loss: 0.04210470851976424\n",
      "Epoch 49\n",
      "Training loss: 0.010146811578096342\n",
      "Validation Loss: 0.03425410173134878\n",
      "Epoch 50\n",
      "Training loss: 0.012592767511696084\n",
      "Validation Loss: 0.045198264475911853\n",
      "Epoch 51\n",
      "Training loss: 0.02798212849602257\n",
      "Validation Loss: 0.05341240718960762\n",
      "Epoch 52\n",
      "Training loss: 0.029169082365565233\n",
      "Validation Loss: 0.05022596742957831\n",
      "Epoch 53\n",
      "Training loss: 0.035234770689582084\n",
      "Validation Loss: 0.071064168587327\n",
      "Epoch 54\n",
      "Training loss: 0.025466156489763064\n",
      "Validation Loss: 0.057989885406568645\n",
      "Epoch 55\n",
      "Training loss: 0.020113956692062087\n",
      "Validation Loss: 0.05569243475794792\n",
      "Epoch 56\n",
      "Training loss: 0.01777517387989102\n",
      "Validation Loss: 0.04607054945081472\n",
      "Epoch 57\n",
      "Training loss: 0.011775271539797181\n",
      "Validation Loss: 0.04588471692055464\n",
      "Epoch 58\n",
      "Training loss: 0.009799733971600834\n",
      "Validation Loss: 0.03733862192369997\n",
      "Epoch 59\n",
      "Training loss: 0.015478060369562242\n",
      "Validation Loss: 0.05836264960467816\n",
      "Epoch 60\n",
      "Training loss: 0.020829932442530223\n",
      "Validation Loss: 0.056642668917775155\n",
      "Epoch 61\n",
      "Training loss: 0.018488081068895067\n",
      "Validation Loss: 0.057218530756654215\n",
      "Epoch 62\n",
      "Training loss: 0.0190117051776921\n",
      "Validation Loss: 0.04740681365132332\n",
      "Epoch 63\n",
      "Training loss: 0.018997216565995335\n",
      "Validation Loss: 0.04280937233939767\n",
      "Epoch 64\n",
      "Training loss: 0.008179175261811344\n",
      "Validation Loss: 0.045495053306221965\n",
      "Epoch 65\n",
      "Training loss: 0.01047522957273519\n",
      "Validation Loss: 0.06760688323527575\n",
      "Epoch 66\n",
      "Training loss: 0.019655119234062348\n",
      "Validation Loss: 0.05556421147659421\n",
      "Epoch 67\n",
      "Training loss: 0.017293353709056207\n",
      "Validation Loss: 0.04972907681018114\n",
      "Epoch 68\n",
      "Training loss: 0.01614015632472241\n",
      "Validation Loss: 0.051343126483261586\n",
      "Epoch 69\n",
      "Training loss: 0.01366270953050056\n",
      "Validation Loss: 0.050510808806866404\n",
      "Epoch 70\n",
      "Training loss: 0.010799087367990274\n",
      "Validation Loss: 0.06874113697558641\n",
      "Epoch 71\n",
      "Training loss: 0.020536157729814656\n",
      "Validation Loss: 0.09913545697927476\n",
      "Epoch 72\n",
      "Training loss: 0.024247382829906707\n",
      "Validation Loss: 0.06825223326683044\n",
      "Epoch 73\n",
      "Training loss: 0.017097147599285104\n",
      "Validation Loss: 0.04634837118908763\n",
      "Epoch 74\n",
      "Training loss: 0.011356377558897911\n",
      "Validation Loss: 0.05395886458456516\n",
      "Epoch 75\n",
      "Training loss: 0.007558278336199289\n",
      "Validation Loss: 0.0371202783100307\n",
      "Epoch 76\n",
      "Training loss: 0.009635239725538827\n",
      "Validation Loss: 0.05195203505456448\n",
      "Epoch 77\n",
      "Training loss: 0.015697818249464035\n",
      "Validation Loss: 0.04875530231744051\n",
      "Epoch 78\n",
      "Training loss: 0.01100792949203133\n",
      "Validation Loss: 0.04110723979771137\n",
      "Epoch 79\n",
      "Training loss: 0.011840685640375331\n",
      "Validation Loss: 0.04056592013686895\n",
      "Epoch 80\n",
      "Training loss: 0.014020592684751934\n",
      "Validation Loss: 0.06513424027711152\n",
      "Epoch 81\n",
      "Training loss: 0.04038845669939039\n",
      "Validation Loss: 0.06621058322489262\n",
      "Epoch 82\n",
      "Training loss: 0.02672593445353901\n",
      "Validation Loss: 0.05392964728496736\n",
      "Epoch 83\n",
      "Training loss: 0.01723166554206118\n",
      "Validation Loss: 0.04069727893918753\n",
      "Epoch 84\n",
      "Training loss: 0.008456045740101602\n",
      "Validation Loss: 0.043266999945044515\n",
      "Epoch 85\n",
      "Training loss: 0.008763243021808335\n",
      "Validation Loss: 0.04422951812855899\n",
      "Epoch 86\n",
      "Training loss: 0.008502805729147009\n",
      "Validation Loss: 0.050557460878044365\n",
      "Epoch 87\n",
      "Training loss: 0.007489769993177087\n",
      "Validation Loss: 0.041453544432297346\n",
      "Epoch 88\n",
      "Training loss: 0.005786332907275172\n",
      "Validation Loss: 0.03646407959982753\n",
      "Epoch 89\n",
      "Training loss: 0.011959005600878411\n",
      "Validation Loss: 0.05578986406326294\n",
      "Epoch 90\n",
      "Training loss: 0.01882790390056433\n",
      "Validation Loss: 0.05870496690273285\n",
      "Epoch 91\n",
      "Training loss: 0.021904590722058238\n",
      "Validation Loss: 0.054485556483268735\n",
      "Epoch 92\n",
      "Training loss: 0.028859059479970906\n",
      "Validation Loss: 0.05806255616247654\n",
      "Epoch 93\n",
      "Training loss: 0.03593068933786498\n",
      "Validation Loss: 0.06848724722862244\n",
      "Epoch 94\n",
      "Training loss: 0.01355851850635612\n",
      "Validation Loss: 0.04988124309107661\n",
      "Epoch 95\n",
      "Training loss: 0.009876249130027964\n",
      "Validation Loss: 0.05294035509228706\n",
      "Epoch 96\n",
      "Training loss: 0.012262876061530611\n",
      "Validation Loss: 0.04801848286297172\n",
      "Epoch 97\n",
      "Training loss: 0.006650760543014202\n",
      "Validation Loss: 0.0366459273151122\n",
      "Epoch 98\n",
      "Training loss: 0.004351910247501063\n",
      "Validation Loss: 0.036107428008690476\n",
      "Epoch 99\n",
      "Training loss: 0.00359007148744212\n",
      "Validation Loss: 0.042162898471578954\n",
      "Epoch 100\n",
      "Training loss: 0.00480718167196268\n",
      "Validation Loss: 0.05257183137349784\n",
      "Finished Training, Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chuan\\AppData\\Local\\Temp\\ipykernel_11812\\1195006754.py:7: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-whitegrid')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4\n",
      "reset weight of layer Linear(in_features=180, out_features=90, bias=True)\n",
      "reset weight of layer Linear(in_features=90, out_features=50, bias=True)\n",
      "reset weight of layer Linear(in_features=50, out_features=1, bias=True)\n",
      "Epoch 1\n",
      "Training loss: 19.528142732443268\n",
      "Validation Loss: 12.42729808807373\n",
      "Epoch 2\n",
      "Training loss: 3.3690061954032515\n",
      "Validation Loss: 0.39708948612213135\n",
      "Epoch 3\n",
      "Training loss: 0.14327947664813898\n",
      "Validation Loss: 0.06395051734056324\n",
      "Epoch 4\n",
      "Training loss: 0.03315628350222694\n",
      "Validation Loss: 0.032611070508137344\n",
      "Epoch 5\n",
      "Training loss: 0.01598971027439249\n",
      "Validation Loss: 0.027475660368800163\n",
      "Epoch 6\n",
      "Training loss: 0.011008128679847134\n",
      "Validation Loss: 0.021897016018629076\n",
      "Epoch 7\n",
      "Training loss: 0.008702630736413844\n",
      "Validation Loss: 0.020069793555885552\n",
      "Epoch 8\n",
      "Training loss: 0.0070867469923129095\n",
      "Validation Loss: 0.017491413569077848\n",
      "Epoch 9\n",
      "Training loss: 0.006759318273476104\n",
      "Validation Loss: 0.01819809722714126\n",
      "Epoch 10\n",
      "Training loss: 0.006431268367282662\n",
      "Validation Loss: 0.018086081966757775\n",
      "Epoch 11\n",
      "Training loss: 0.006146069961724822\n",
      "Validation Loss: 0.01878436591476202\n",
      "Epoch 12\n",
      "Training loss: 0.006971359549368703\n",
      "Validation Loss: 0.01979413066059351\n",
      "Epoch 13\n",
      "Training loss: 0.006146735181676743\n",
      "Validation Loss: 0.022815061658620836\n",
      "Epoch 14\n",
      "Training loss: 0.007308597844478088\n",
      "Validation Loss: 0.019798562694340945\n",
      "Epoch 15\n",
      "Training loss: 0.0070597171248176814\n",
      "Validation Loss: 0.016764023825526237\n",
      "Epoch 16\n",
      "Training loss: 0.006681886011945833\n",
      "Validation Loss: 0.01744277634541504\n",
      "Epoch 17\n",
      "Training loss: 0.008532267615928785\n",
      "Validation Loss: 0.022613178510218858\n",
      "Epoch 18\n",
      "Training loss: 0.013939122069319807\n",
      "Validation Loss: 0.03274006064981222\n",
      "Epoch 19\n",
      "Training loss: 0.027889396167197024\n",
      "Validation Loss: 0.039667815398424865\n",
      "Epoch 20\n",
      "Training loss: 0.050855466039832105\n",
      "Validation Loss: 0.053442597426474094\n",
      "Epoch 21\n",
      "Training loss: 0.05332916467114515\n",
      "Validation Loss: 0.07048536904156208\n",
      "Epoch 22\n",
      "Training loss: 0.043173164993370934\n",
      "Validation Loss: 0.03713595813140273\n",
      "Epoch 23\n",
      "Training loss: 0.03152771055525572\n",
      "Validation Loss: 0.036709254193119704\n",
      "Epoch 24\n",
      "Training loss: 0.02127636317643769\n",
      "Validation Loss: 0.02937224119901657\n",
      "Epoch 25\n",
      "Training loss: 0.014006248356044753\n",
      "Validation Loss: 0.020251895189285277\n",
      "Epoch 26\n",
      "Training loss: 0.011920328860497582\n",
      "Validation Loss: 0.024714813143364153\n",
      "Epoch 27\n",
      "Training loss: 0.010391235709209572\n",
      "Validation Loss: 0.0227707183547318\n",
      "Epoch 28\n",
      "Training loss: 0.01293605101563651\n",
      "Validation Loss: 0.029214418880292213\n",
      "Epoch 29\n",
      "Training loss: 0.013642567687562293\n",
      "Validation Loss: 0.02633700697682798\n",
      "Epoch 30\n",
      "Training loss: 0.01553506629791149\n",
      "Validation Loss: 0.030739259738475085\n",
      "Epoch 31\n",
      "Training loss: 0.016425034172890575\n",
      "Validation Loss: 0.03886198647320271\n",
      "Epoch 32\n",
      "Training loss: 0.02999855393601447\n",
      "Validation Loss: 0.03808621171861887\n",
      "Epoch 33\n",
      "Training loss: 0.02720812457546438\n",
      "Validation Loss: 0.05258992575109005\n",
      "Epoch 34\n",
      "Training loss: 0.02826535131916711\n",
      "Validation Loss: 0.06464179866015911\n",
      "Epoch 35\n",
      "Training loss: 0.0438237034130035\n",
      "Validation Loss: 0.07408965043723584\n",
      "Epoch 36\n",
      "Training loss: 0.042045532908175406\n",
      "Validation Loss: 0.045196774788200854\n",
      "Epoch 37\n",
      "Training loss: 0.017418613364516767\n",
      "Validation Loss: 0.03458252942189574\n",
      "Epoch 38\n",
      "Training loss: 0.01163337746222225\n",
      "Validation Loss: 0.02546054819598794\n",
      "Epoch 39\n",
      "Training loss: 0.009403189693423967\n",
      "Validation Loss: 0.02822485636919737\n",
      "Epoch 40\n",
      "Training loss: 0.010246145870229326\n",
      "Validation Loss: 0.032165029030293224\n",
      "Epoch 41\n",
      "Training loss: 0.02130389999905505\n",
      "Validation Loss: 0.05302437655540416\n",
      "Epoch 42\n",
      "Training loss: 0.04146664936249096\n",
      "Validation Loss: 0.0612365117110312\n",
      "Epoch 43\n",
      "Training loss: 0.023707356615971353\n",
      "Validation Loss: 0.02912771054543555\n",
      "Epoch 44\n",
      "Training loss: 0.015937559162602595\n",
      "Validation Loss: 0.03249742470681667\n",
      "Epoch 45\n",
      "Training loss: 0.012260662262164747\n",
      "Validation Loss: 0.025694424882531167\n",
      "Epoch 46\n",
      "Training loss: 0.009980444796383381\n",
      "Validation Loss: 0.020783539349213242\n",
      "Epoch 47\n",
      "Training loss: 0.009042220677431557\n",
      "Validation Loss: 0.02675808612257242\n",
      "Epoch 48\n",
      "Training loss: 0.016814292074882033\n",
      "Validation Loss: 0.034226290965452794\n",
      "Epoch 49\n",
      "Training loss: 0.016366580681222462\n",
      "Validation Loss: 0.03985122807323933\n",
      "Epoch 50\n",
      "Training loss: 0.0178951262832456\n",
      "Validation Loss: 0.03434626631205902\n",
      "Epoch 51\n",
      "Training loss: 0.014556763806505148\n",
      "Validation Loss: 0.029870586562901737\n",
      "Epoch 52\n",
      "Training loss: 0.018192831095615307\n",
      "Validation Loss: 0.04208634749054909\n",
      "Epoch 53\n",
      "Training loss: 0.023397032613145935\n",
      "Validation Loss: 0.03922783240675926\n",
      "Epoch 54\n",
      "Training loss: 0.01767371538738461\n",
      "Validation Loss: 0.04105588044971228\n",
      "Epoch 55\n",
      "Training loss: 0.018275094989053522\n",
      "Validation Loss: 0.036512095239013435\n",
      "Epoch 56\n",
      "Training loss: 0.010605575977680609\n",
      "Validation Loss: 0.027346263639628887\n",
      "Epoch 57\n",
      "Training loss: 0.012287720674216823\n",
      "Validation Loss: 0.03078581888228655\n",
      "Epoch 58\n",
      "Training loss: 0.008706307921981075\n",
      "Validation Loss: 0.02533567454665899\n",
      "Epoch 59\n",
      "Training loss: 0.013744964261332853\n",
      "Validation Loss: 0.03532605180516839\n",
      "Epoch 60\n",
      "Training loss: 0.025848904761399345\n",
      "Validation Loss: 0.039293307736516\n",
      "Epoch 61\n",
      "Training loss: 0.031649711589837815\n",
      "Validation Loss: 0.048167776856571434\n",
      "Epoch 62\n",
      "Training loss: 0.019254459183394294\n",
      "Validation Loss: 0.03412613354623318\n",
      "Epoch 63\n",
      "Training loss: 0.017464415417484863\n",
      "Validation Loss: 0.03328755795024335\n",
      "Epoch 64\n",
      "Training loss: 0.01345268145117179\n",
      "Validation Loss: 0.027914165472611784\n",
      "Epoch 65\n",
      "Training loss: 0.009182782378047705\n",
      "Validation Loss: 0.028896295074373482\n",
      "Epoch 66\n",
      "Training loss: 0.012169135873657219\n",
      "Validation Loss: 0.040727145113050936\n",
      "Epoch 67\n",
      "Training loss: 0.010905357204934525\n",
      "Validation Loss: 0.02776112040504813\n",
      "Epoch 68\n",
      "Training loss: 0.011098197067262047\n",
      "Validation Loss: 0.04202691797167063\n",
      "Epoch 69\n",
      "Training loss: 0.012744902609127392\n",
      "Validation Loss: 0.03721724569797516\n",
      "Epoch 70\n",
      "Training loss: 0.020339372851069747\n",
      "Validation Loss: 0.03543363898992538\n",
      "Epoch 71\n",
      "Training loss: 0.014714666853476431\n",
      "Validation Loss: 0.030300504956394435\n",
      "Epoch 72\n",
      "Training loss: 0.021957062376835913\n",
      "Validation Loss: 0.036444747745990755\n",
      "Epoch 73\n",
      "Training loss: 0.020745489237465196\n",
      "Validation Loss: 0.041882189493626355\n",
      "Epoch 74\n",
      "Training loss: 0.018080978555912057\n",
      "Validation Loss: 0.04100128926336765\n",
      "Epoch 75\n",
      "Training loss: 0.013761896054385249\n",
      "Validation Loss: 0.0321916176751256\n",
      "Epoch 76\n",
      "Training loss: 0.012691465093629416\n",
      "Validation Loss: 0.03355717074126005\n",
      "Epoch 77\n",
      "Training loss: 0.01035007930927209\n",
      "Validation Loss: 0.02960726972669363\n",
      "Epoch 78\n",
      "Training loss: 0.012949788462869899\n",
      "Validation Loss: 0.045398713387548924\n",
      "Epoch 79\n",
      "Training loss: 0.019177825194927528\n",
      "Validation Loss: 0.04098821964114904\n",
      "Epoch 80\n",
      "Training loss: 0.014989498340207892\n",
      "Validation Loss: 0.05155261915177107\n",
      "Epoch 81\n",
      "Training loss: 0.014833981406481303\n",
      "Validation Loss: 0.027828493807464837\n",
      "Epoch 82\n",
      "Training loss: 0.011718746538744452\n",
      "Validation Loss: 0.03288490867242217\n",
      "Epoch 83\n",
      "Training loss: 0.006778617276685293\n",
      "Validation Loss: 0.03690411301329732\n",
      "Epoch 84\n",
      "Training loss: 0.008151924330021073\n",
      "Validation Loss: 0.02901426550000906\n",
      "Epoch 85\n",
      "Training loss: 0.011011641522984682\n",
      "Validation Loss: 0.03670489678159356\n",
      "Epoch 86\n",
      "Training loss: 0.03137399374324944\n",
      "Validation Loss: 0.05496672995388508\n",
      "Epoch 87\n",
      "Training loss: 0.019019837251182684\n",
      "Validation Loss: 0.0423014247789979\n",
      "Epoch 88\n",
      "Training loss: 0.012993618087439807\n",
      "Validation Loss: 0.036517521180212495\n",
      "Epoch 89\n",
      "Training loss: 0.00887678807466915\n",
      "Validation Loss: 0.0302956017665565\n",
      "Epoch 90\n",
      "Training loss: 0.010742036786564078\n",
      "Validation Loss: 0.03053221954032779\n",
      "Epoch 91\n",
      "Training loss: 0.009423578525269307\n",
      "Validation Loss: 0.028221765514463187\n",
      "Epoch 92\n",
      "Training loss: 0.006337565177878768\n",
      "Validation Loss: 0.028479312052950262\n",
      "Epoch 93\n",
      "Training loss: 0.006137291790399048\n",
      "Validation Loss: 0.02557621059473604\n",
      "Epoch 94\n",
      "Training loss: 0.005737964653160385\n",
      "Validation Loss: 0.028662373665720223\n",
      "Epoch 95\n",
      "Training loss: 0.0061025108371563636\n",
      "Validation Loss: 0.03264064956456423\n",
      "Epoch 96\n",
      "Training loss: 0.028354112510137336\n",
      "Validation Loss: 0.05687316782772541\n",
      "Epoch 97\n",
      "Training loss: 0.0359155168881659\n",
      "Validation Loss: 0.060451101362705234\n",
      "Epoch 98\n",
      "Training loss: 0.02198761340576349\n",
      "Validation Loss: 0.03351209629327059\n",
      "Epoch 99\n",
      "Training loss: 0.010527610865219967\n",
      "Validation Loss: 0.03309721551835537\n",
      "Epoch 100\n",
      "Training loss: 0.010105783040587282\n",
      "Validation Loss: 0.04235927837900817\n",
      "Finished Training, Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chuan\\AppData\\Local\\Temp\\ipykernel_11812\\1195006754.py:7: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-whitegrid')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5\n",
      "reset weight of layer Linear(in_features=180, out_features=90, bias=True)\n",
      "reset weight of layer Linear(in_features=90, out_features=50, bias=True)\n",
      "reset weight of layer Linear(in_features=50, out_features=1, bias=True)\n",
      "Epoch 1\n",
      "Training loss: 18.97559109913934\n",
      "Validation Loss: 12.01240493774414\n",
      "Epoch 2\n",
      "Training loss: 2.742833489609748\n",
      "Validation Loss: 0.44186893284320833\n",
      "Epoch 3\n",
      "Training loss: 0.16585435058691145\n",
      "Validation Loss: 0.07180320173501968\n",
      "Epoch 4\n",
      "Training loss: 0.03601938929677624\n",
      "Validation Loss: 0.0345240618288517\n",
      "Epoch 5\n",
      "Training loss: 0.015266281005340753\n",
      "Validation Loss: 0.024754621712490918\n",
      "Epoch 6\n",
      "Training loss: 0.012428696598550401\n",
      "Validation Loss: 0.02344749428331852\n",
      "Epoch 7\n",
      "Training loss: 0.009547497793916882\n",
      "Validation Loss: 0.020859901942312718\n",
      "Epoch 8\n",
      "Training loss: 0.008553236905370177\n",
      "Validation Loss: 0.01833166124051786\n",
      "Epoch 9\n",
      "Training loss: 0.006541175474940809\n",
      "Validation Loss: 0.01824421134777367\n",
      "Epoch 10\n",
      "Training loss: 0.005620689235733265\n",
      "Validation Loss: 0.01597820356488228\n",
      "Epoch 11\n",
      "Training loss: 0.005683290350884583\n",
      "Validation Loss: 0.01789433461613953\n",
      "Epoch 12\n",
      "Training loss: 0.004902079355778154\n",
      "Validation Loss: 0.01737870197736811\n",
      "Epoch 13\n",
      "Training loss: 0.00485517508854379\n",
      "Validation Loss: 0.018556549502536653\n",
      "Epoch 14\n",
      "Training loss: 0.007576634861365652\n",
      "Validation Loss: 0.01887997942045331\n",
      "Epoch 15\n",
      "Training loss: 0.010915371704577785\n",
      "Validation Loss: 0.019183897422626616\n",
      "Epoch 16\n",
      "Training loss: 0.016045169844820174\n",
      "Validation Loss: 0.027553405500948428\n",
      "Epoch 17\n",
      "Training loss: 0.029787979736808957\n",
      "Validation Loss: 0.03903232913464308\n",
      "Epoch 18\n",
      "Training loss: 0.0313365944257947\n",
      "Validation Loss: 0.04162969865836203\n",
      "Epoch 19\n",
      "Training loss: 0.026259524224460432\n",
      "Validation Loss: 0.04222646661102772\n",
      "Epoch 20\n",
      "Training loss: 0.030937450193828837\n",
      "Validation Loss: 0.048942062556743625\n",
      "Epoch 21\n",
      "Training loss: 0.030839651789439402\n",
      "Validation Loss: 0.04709236294031143\n",
      "Epoch 22\n",
      "Training loss: 0.03532378867114942\n",
      "Validation Loss: 0.039103065775707366\n",
      "Epoch 23\n",
      "Training loss: 0.022327106426825227\n",
      "Validation Loss: 0.03091771263629198\n",
      "Epoch 24\n",
      "Training loss: 0.012201114528879677\n",
      "Validation Loss: 0.025433934889733792\n",
      "Epoch 25\n",
      "Training loss: 0.013661907652162552\n",
      "Validation Loss: 0.022632292695343493\n",
      "Epoch 26\n",
      "Training loss: 0.013611940791849624\n",
      "Validation Loss: 0.032402561381459236\n",
      "Epoch 27\n",
      "Training loss: 0.016590725832148313\n",
      "Validation Loss: 0.02989167831838131\n",
      "Epoch 28\n",
      "Training loss: 0.018109993466665756\n",
      "Validation Loss: 0.03421368129551411\n",
      "Epoch 29\n",
      "Training loss: 0.036043965296109315\n",
      "Validation Loss: 0.06376531690359116\n",
      "Epoch 30\n",
      "Training loss: 0.03650159237080628\n",
      "Validation Loss: 0.03332504853606224\n",
      "Epoch 31\n",
      "Training loss: 0.027171746818060727\n",
      "Validation Loss: 0.05754336029291153\n",
      "Epoch 32\n",
      "Training loss: 0.035635019268494904\n",
      "Validation Loss: 0.04499912366271019\n",
      "Epoch 33\n",
      "Training loss: 0.02089798254844224\n",
      "Validation Loss: 0.022353991074487566\n",
      "Epoch 34\n",
      "Training loss: 0.021783754822423625\n",
      "Validation Loss: 0.04077056962996721\n",
      "Epoch 35\n",
      "Training loss: 0.017756313950869917\n",
      "Validation Loss: 0.028132269755005837\n",
      "Epoch 36\n",
      "Training loss: 0.015504752662628121\n",
      "Validation Loss: 0.034579341895878316\n",
      "Epoch 37\n",
      "Training loss: 0.014678623217647685\n",
      "Validation Loss: 0.023294983338564633\n",
      "Epoch 38\n",
      "Training loss: 0.013874140613683566\n",
      "Validation Loss: 0.033555123563855885\n",
      "Epoch 39\n",
      "Training loss: 0.021571431544215716\n",
      "Validation Loss: 0.04564757715910673\n",
      "Epoch 40\n",
      "Training loss: 0.02270235556187396\n",
      "Validation Loss: 0.02424562372267246\n",
      "Epoch 41\n",
      "Training loss: 0.013497608432968714\n",
      "Validation Loss: 0.024689008090645073\n",
      "Epoch 42\n",
      "Training loss: 0.01305122150242636\n",
      "Validation Loss: 0.02829210415482521\n",
      "Epoch 43\n",
      "Training loss: 0.02536737034097314\n",
      "Validation Loss: 0.05595639891922474\n",
      "Epoch 44\n",
      "Training loss: 0.03529021807840651\n",
      "Validation Loss: 0.0361218386143446\n",
      "Epoch 45\n",
      "Training loss: 0.022874172508102104\n",
      "Validation Loss: 0.031005510012619197\n",
      "Epoch 46\n",
      "Training loss: 0.018087271701774953\n",
      "Validation Loss: 0.030766869261860848\n",
      "Epoch 47\n",
      "Training loss: 0.011472179422385454\n",
      "Validation Loss: 0.023700905460864306\n",
      "Epoch 48\n",
      "Training loss: 0.008891084142624564\n",
      "Validation Loss: 0.02346759877167642\n",
      "Epoch 49\n",
      "Training loss: 0.008013640182361621\n",
      "Validation Loss: 0.02346237000543624\n",
      "Epoch 50\n",
      "Training loss: 0.013489570420657852\n",
      "Validation Loss: 0.054252152442932126\n",
      "Epoch 51\n",
      "Training loss: 0.03573099883830117\n",
      "Validation Loss: 0.06714539099484682\n",
      "Epoch 52\n",
      "Training loss: 0.03277026608432691\n",
      "Validation Loss: 0.0428638378821779\n",
      "Epoch 53\n",
      "Training loss: 0.023549508322606383\n",
      "Validation Loss: 0.030843578884378074\n",
      "Epoch 54\n",
      "Training loss: 0.014545580492230113\n",
      "Validation Loss: 0.029969230741262436\n",
      "Epoch 55\n",
      "Training loss: 0.009508459433942035\n",
      "Validation Loss: 0.03605663888156414\n",
      "Epoch 56\n",
      "Training loss: 0.03009996452301587\n",
      "Validation Loss: 0.040480220839381215\n",
      "Epoch 57\n",
      "Training loss: 0.020704804530808914\n",
      "Validation Loss: 0.021659236270934343\n",
      "Epoch 58\n",
      "Training loss: 0.016005018497961238\n",
      "Validation Loss: 0.027590886326506733\n",
      "Epoch 59\n",
      "Training loss: 0.013081200941725029\n",
      "Validation Loss: 0.026920431554317475\n",
      "Epoch 60\n",
      "Training loss: 0.014207645129777262\n",
      "Validation Loss: 0.04272577049210668\n",
      "Epoch 61\n",
      "Training loss: 0.014152044555191527\n",
      "Validation Loss: 0.02592608643695712\n",
      "Epoch 62\n",
      "Training loss: 0.01774500343785381\n",
      "Validation Loss: 0.05375087957829237\n",
      "Epoch 63\n",
      "Training loss: 0.030359950636695957\n",
      "Validation Loss: 0.03776209440024104\n",
      "Epoch 64\n",
      "Training loss: 0.030824522305395185\n",
      "Validation Loss: 0.04421790645457804\n",
      "Epoch 65\n",
      "Training loss: 0.01531636244233352\n",
      "Validation Loss: 0.032229821979999546\n",
      "Epoch 66\n",
      "Training loss: 0.008187566005346394\n",
      "Validation Loss: 0.023344578305259347\n",
      "Epoch 67\n",
      "Training loss: 0.007366940554365823\n",
      "Validation Loss: 0.02175199851510115\n",
      "Epoch 68\n",
      "Training loss: 0.009470852456756473\n",
      "Validation Loss: 0.023403682957869022\n",
      "Epoch 69\n",
      "Training loss: 0.009175790413810881\n",
      "Validation Loss: 0.021076253028586508\n",
      "Epoch 70\n",
      "Training loss: 0.006016281338591014\n",
      "Validation Loss: 0.021400752868503334\n",
      "Epoch 71\n",
      "Training loss: 0.006001533216739208\n",
      "Validation Loss: 0.024855200545280242\n",
      "Epoch 72\n",
      "Training loss: 0.0102716738540564\n",
      "Validation Loss: 0.02602024002932012\n",
      "Epoch 73\n",
      "Training loss: 0.016053453445453773\n",
      "Validation Loss: 0.028947586527792737\n",
      "Epoch 74\n",
      "Training loss: 0.0393703832703921\n",
      "Validation Loss: 0.04218726940453053\n",
      "Epoch 75\n",
      "Training loss: 0.02161132529877203\n",
      "Validation Loss: 0.03812250762246549\n",
      "Epoch 76\n",
      "Training loss: 0.016426313294040172\n",
      "Validation Loss: 0.03560475546866655\n",
      "Epoch 77\n",
      "Training loss: 0.019648120344914113\n",
      "Validation Loss: 0.03291550161316991\n",
      "Epoch 78\n",
      "Training loss: 0.011333038418352143\n",
      "Validation Loss: 0.026817382695153356\n",
      "Epoch 79\n",
      "Training loss: 0.009554541398550278\n",
      "Validation Loss: 0.02706354284659028\n",
      "Epoch 80\n",
      "Training loss: 0.005443401968337058\n",
      "Validation Loss: 0.021017026649788023\n",
      "Epoch 81\n",
      "Training loss: 0.004621053763818879\n",
      "Validation Loss: 0.02347846540622413\n",
      "Epoch 82\n",
      "Training loss: 0.008700731437240448\n",
      "Validation Loss: 0.021430465597004515\n",
      "Epoch 83\n",
      "Training loss: 0.007996050150076062\n",
      "Validation Loss: 0.026546762939542532\n",
      "Epoch 84\n",
      "Training loss: 0.013147475001085358\n",
      "Validation Loss: 0.03471859494224191\n",
      "Epoch 85\n",
      "Training loss: 0.0110851691426114\n",
      "Validation Loss: 0.031044457294046878\n",
      "Epoch 86\n",
      "Training loss: 0.015807835121168613\n",
      "Validation Loss: 0.03298550609499216\n",
      "Epoch 87\n",
      "Training loss: 0.019616822846541086\n",
      "Validation Loss: 0.043993107376154514\n",
      "Epoch 88\n",
      "Training loss: 0.018732114286958864\n",
      "Validation Loss: 0.04642895862692967\n",
      "Epoch 89\n",
      "Training loss: 0.0167627398611152\n",
      "Validation Loss: 0.030219886950217188\n",
      "Epoch 90\n",
      "Training loss: 0.019419190819500033\n",
      "Validation Loss: 0.047075063663069155\n",
      "Epoch 91\n",
      "Training loss: 0.01878808381650405\n",
      "Validation Loss: 0.026671647895127534\n",
      "Epoch 92\n",
      "Training loss: 0.00949709610917519\n",
      "Validation Loss: 0.024684796649962662\n",
      "Epoch 93\n",
      "Training loss: 0.010623681963872663\n",
      "Validation Loss: 0.04137969192117453\n",
      "Epoch 94\n",
      "Training loss: 0.011435200897112642\n",
      "Validation Loss: 0.03138777593150735\n",
      "Epoch 95\n",
      "Training loss: 0.012431541846623434\n",
      "Validation Loss: 0.03373115140944719\n",
      "Epoch 96\n",
      "Training loss: 0.012727919125871867\n",
      "Validation Loss: 0.028795965011231602\n",
      "Epoch 97\n",
      "Training loss: 0.010176802464501606\n",
      "Validation Loss: 0.02696688137948513\n",
      "Epoch 98\n",
      "Training loss: 0.013230632481731704\n",
      "Validation Loss: 0.03714214541018009\n",
      "Epoch 99\n",
      "Training loss: 0.027825857018187794\n",
      "Validation Loss: 0.03391698209568858\n",
      "Epoch 100\n",
      "Training loss: 0.024047671846046892\n",
      "Validation Loss: 0.03164171196520329\n",
      "Finished Training, Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chuan\\AppData\\Local\\Temp\\ipykernel_11812\\1195006754.py:7: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-whitegrid')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(69)\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "print(\"Starting KFold Cross Validation\")\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(peptides_dataset), 1):\n",
    "    \n",
    "    print(f'Fold {fold}')\n",
    "\n",
    "    # Shuffle the data\n",
    "    train_subsampler = SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = SubsetRandomSampler(test_ids)\n",
    "\n",
    "    # Define the data loaders\n",
    "    train_loader = DataLoader(peptides_dataset, batch_size=10, sampler=train_subsampler)\n",
    "    test_loader = DataLoader(peptides_dataset, batch_size=10, sampler=test_subsampler)\n",
    "\n",
    "    # Initialize NN\n",
    "    model = Linear_NN().to(device)\n",
    "    model.apply(reset_weights)\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    for epoch in range(0, 100):\n",
    "    \n",
    "            print(f'Epoch {epoch+1}')\n",
    "\n",
    "            training_loss = 0.0\n",
    "\n",
    "            for i, data in enumerate(train_loader):\n",
    "                  \n",
    "                # Get the inputs\n",
    "                inputs, labels = data\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs.view(-1, 180))\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Optimize\n",
    "                optimizer.step()\n",
    "\n",
    "                training_loss += loss.item()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                validation_loss = 0.0\n",
    "\n",
    "                for inputs, labels in test_loader:\n",
    "\n",
    "                    outputs = model(inputs.view(-1, 180))\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    validation_loss += loss.item()\n",
    "\n",
    "            training_losses.append(training_loss/len(train_loader))\n",
    "            validation_losses.append(validation_loss/len(test_loader))\n",
    "\n",
    "            print(f'Training loss: {training_loss/len(train_loader)}')\n",
    "            print(f'Validation Loss: {validation_loss/len(test_loader)}')\n",
    "\n",
    "            \n",
    "    # Save the model\n",
    "    print('Finished Training, Saving Model')\n",
    "    torch.save(model.state_dict(), f'./model/model_fold_{fold}.pt')\n",
    "\n",
    "    # Plot the loss\n",
    "    plot_loss(training_losses, validation_losses, fold) \n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model with unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss: 0.012524592690169811\n"
     ]
    }
   ],
   "source": [
    "evaluation_data = './data/evaluation_peptide_PSSM_scores.txt'\n",
    "# evaluation_data = '../data/data/PSSM/A0201.eval'\n",
    "evaluation_peptides, evaluation_score = load_peptide_data(evaluation_data)\n",
    "evaluation_one_hot_encoded_peptides = one_hot_encode_peptides(evaluation_peptides)\n",
    "\n",
    "# Convert to tensors\n",
    "evaluation_peptides_tensor = torch.tensor(evaluation_one_hot_encoded_peptides, dtype=torch.float32).to(device)\n",
    "evaluation_score_tensor = torch.tensor(np.asarray(evaluation_score).reshape(-1,1), dtype=torch.float32).to(device)\n",
    "\n",
    "evaluation_peptides_dataset = TensorDataset(evaluation_peptides_tensor, evaluation_score_tensor)\n",
    "\n",
    "\n",
    "def test_model(models, evaluation_peptides_dataset):\n",
    "    predictions = []\n",
    "\n",
    "    evaluation_data_loader = DataLoader(evaluation_peptides_dataset, batch_size=10)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            model_predictions = []\n",
    "\n",
    "            for peptides, _ in evaluation_data_loader:\n",
    "                peptides = peptides.to(device)\n",
    "                outputs = model(peptides.view(-1, 180))\n",
    "                model_predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "            predictions.append(model_predictions)\n",
    "\n",
    "    # Average predictions across models\n",
    "    averaged_predictions = np.mean(predictions, axis=0)\n",
    "\n",
    "    # Calculate total loss\n",
    "    total_loss = criterion(torch.tensor(averaged_predictions, dtype=torch.float32).to(device), evaluation_score_tensor).item()\n",
    "\n",
    "    return averaged_predictions, total_loss\n",
    "\n",
    "# Load models\n",
    "models = []\n",
    "for i in range(1,6):\n",
    "    model = Linear_NN().to(device)\n",
    "    model.load_state_dict(torch.load(f'./model/model_fold_{i}.pt'))\n",
    "    models.append(model)\n",
    "\n",
    "# Test models\n",
    "predictions, total_loss = test_model(models, evaluation_peptides_dataset)\n",
    "\n",
    "print(f'Total Loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving evaluation data prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = './evaluation_result/evaluation_predictions.txt'\n",
    "\n",
    "with open(outfile, 'w') as f:\n",
    "    print('Peptide      Score      Prediction', file=f)\n",
    "    for peptide, score, prediction in zip(evaluation_peptides, evaluation_score, predictions):\n",
    "        print(f'{\"\".join(peptide):<12} {score:<10.4f} {prediction[0]:<10.4f}', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCC:  0.999757426253449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chuan\\AppData\\Local\\Temp\\ipykernel_11812\\634076704.py:5: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-whitegrid');\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pcc = pearsonr(evaluation_score, np.array(predictions).flatten())\n",
    "print(\"PCC: \", pcc[0])\n",
    "\n",
    "plt.figure()\n",
    "plt.style.use('seaborn-whitegrid');\n",
    "plt.scatter(evaluation_score, predictions, edgecolors='black');\n",
    "plt.xlabel('Target PSSM');\n",
    "plt.ylabel('Predicted PSSM');\n",
    "plt.savefig('./evaluation_result/evaluation_scatter.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
